import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from mpl_toolkits.axes_grid1 import make_axes_locatable
import os

def retrieve_cds_cmip6(c, variable, variable_short, model, experiment, level, t_resolution, data_format, download_directory):
    """
    Download CMIP6 data from Climate Data Store (https://cds.climate.copernicus.eu/cdsapp#!/dataset/projections-cmip6?tab=overview).

    Parameters
    ----------
    variable : string
        Name of the variable to download (e.g., 'near_surface_air_temperature')
    variable_short : string (e.g., 'tas')
        Shortened name of the variable to download
    model : string
        Name of the Earth System Model (e.g., 'access_cm2', 'awi_cm_1_1_mr')
    experiment : list
        List of strings containing the name of the experiment (e.g., ['historical', 'ssp2_4_5', 'ssp3_7_0', 'ssp5_8_5'])
    level : string
        Pressure level of the variable to download (e.g., 'single_levels', '1', '50')
    t_resolution : string
        Temporal resolution of the data to dowanload
    data_format : string 
        Format of the data to download (e.g., 'zip')
    download_directory : string
        Path of the directory where downloaded data have to be saved

    Returns
    -------
    -
    """
    print(f'\nDownloading {model} - {experiment} ...')
    if not os.path.isdir(download_directory):
        os.makedirs(download_directory)
    
    download_file = f'{download_directory}/{variable_short}-{model}-{experiment}.zip'
    
    c.retrieve(
    'projections-cmip6',
    {
        'temporal_resolution': t_resolution,
        'experiment': experiment,
        'level': level,
        'variable': variable,
        'model': model,
        'format': data_format,
    },
    download_file)

def plot_prediction_mae_map(val_y, val_y_pred, model_name, scenario, epoch, val_year, PATH_TO_SAVE_IMG):
    """
    Plot DNN prediction, CMIP6 simulation and MAE temperature maps.

    Parameters
    ----------
        val_y: ndarray
            2D array containing the CMIP6 map generated by the model
        val_y_pred : ndarray
            2D array containing the map predicted by the DNN
        model_name : string
            Name of the model that generated the map and the DNN was trained on
        scenario : string
            Name of the SSP projection scenario
        epoch : int
            Number of the epoch in which the map was generated by the DNN
        val_year : int
            Validation year in which the map was generated by the DNN
        PATH_TO_SAVE_IMG : str
            Absolute path (including directory and filename in which the image has to be saved)

    Returns
    -------
        -
    """
    val_predictions_error = val_y_pred[:,:]-val_y[:,:]
    val_predictions_absolute_error = np.abs(val_predictions_error)
    val_predictions_rmse = np.sqrt(np.mean(val_predictions_error**2, axis=(0,1)))
    val_predictions_mae = val_predictions_absolute_error.mean(axis=(0,1))

    min_temp = np.concatenate((val_y, val_y_pred)).min()
    max_temp = np.concatenate((val_y, val_y_pred)).max()

    fig = plt.figure(figsize=(20,13))
    fig.suptitle(f'Near surface air temperature\n\nModel: {model_name} - Scenario: {scenario} - Epoch: {epoch} - Year: {str(val_year)}', y=0.76, fontsize=14)
    fig.subplots_adjust(top=0.97, wspace=0.10, hspace = 0.2)

    ax1 = fig.add_subplot(1, 3, 1)
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax1.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax1.set_title('NN prediction', loc='center')
    img1 = ax1.imshow(val_y_pred[:,:], interpolation='nearest', origin = 'lower', vmin=min_temp, vmax=max_temp)
    divider = make_axes_locatable(ax1)
    cax1 = divider.append_axes('bottom', size='5%', pad='5%')
    cax1.xaxis.set_ticks_position("bottom")
    cax1.xaxis.labelpad = 5
    ax1.set_xticks([])
    ax1.set_yticks([])
    fig.colorbar(img1, cax=cax1, orientation='horizontal', label="[K]")

    ax2 = fig.add_subplot(1, 3, 2)
    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax2.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax2.set_title('CMIP6 model simulation (Ground Truth)', loc='center')
    img2 = ax2.imshow(val_y[:,:], interpolation='nearest', origin = 'lower', vmin=min_temp, vmax=max_temp)
    divider = make_axes_locatable(ax2)
    cax2 = divider.append_axes('bottom', size='5%', pad='5%')
    cax2.xaxis.set_ticks_position("bottom")
    cax2.xaxis.labelpad = 5
    ax2.set_xticks([])
    ax2.set_yticks([])
    fig.colorbar(img2, cax=cax2, orientation='horizontal', label="[K]")

    ax3 = fig.add_subplot(1, 3, 3)
    ax3.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax3.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax3.set_title(f'RMSE: {round(val_predictions_rmse,2)} - MAE: {round(val_predictions_mae,2)}', loc='center')
    img3 = ax3.imshow(val_predictions_absolute_error[:,:], interpolation='nearest', origin='lower')
    divider = make_axes_locatable(ax3)
    cax3 = divider.append_axes('bottom', size='5%', pad='5%')
    cax3.xaxis.set_ticks_position("bottom")
    cax3.xaxis.labelpad = 10
    ax3.set_xticks([])
    ax3.set_yticks([])
    fig.colorbar(img3, cax=cax3, orientation='horizontal', label="[K]")

    plt.savefig(PATH_TO_SAVE_IMG, bbox_inches = 'tight', dpi=300)
    plt.close()

def normalize_img(data, lower, upper, glob_min, glob_max):
    """
    Translate data into the range [lower, upper].

    Parameters
    ----------
    data : ndarray
        Array containing data to be normalized
    lower : int
        Lower end of the normalization range
    upper : int
        Upper end of the normalization range
    glob_min : float
        Highest data value
    glob_max : float
        Lowest data value

    Returns
    -------
        ndarray containing normalized data
    """
    return (((data - glob_min)/(glob_max - glob_min))*(upper-lower) + lower)

def denormalize_img(data, lower, upper, glob_min, glob_max):
    """
    Translate normalized data into the original range [glob_min, glob_max].

    Parameters
    ----------
    data : ndarray
        Array containing normalized data
    lower : int
        Lower end of the normalized range
    upper : int
        Upper end of the normalized range
    glob_min : float
        Lower end of the original range
    glob_max : float
        Higher end of te original range
    Returns
    -------
        ndarray containing data in the original range
    """
    return (((data - lower)/(upper - lower))*(glob_max - glob_min) + glob_min)

def plot_train_val_loss_curve(train_loss, val_loss, loss, path_to_save):
    """
    Plot train and val loss curves.

    Parameters
    ----------
        train_loss: ndarray
        val_loss : ndarray
        loss : string
            String indicating loss used (e.g. 'mae' or 'mse')
        path_to_save : str
            Absolute path (including directory and filename in which the image has to be saved)

    Returns
    -------
        -
    """
    if loss == 'mae':
        loss_label = 'MAE loss'
    elif loss == 'mse':
        loss_label = 'MSE loss'

    fig, ax = plt.subplots(1, figsize=(8,6))
    num_epochs = len(train_loss)

    ax.plot(np.arange(0, num_epochs), train_loss, label=f'Training {loss_label}')
    
    # val_loss is None we don't have a validation set, thus all years are destined to the training set
    if val_loss != None:
        ax.plot(np.arange(0, num_epochs), val_loss, label=f'Validation {loss_label}')
    ax.legend()

    plt.tight_layout()

    plt.savefig(path_to_save, dpi=300)
    plt.close()

def sets_setup(X, y, splitting_ratio=(0.7,0.8), shuffle=(True, 42)):
    """
    Return the training, validation and test sets.

    Parameters
    ----------
    X : list
        List of float values representing the input for the DNN
    y : ndarray
        3D array containing the maps representing the target for the DNN.
        
        `shape: (n,rows,cols)`
    splitting_ratio : tuple
        Tuple of two float values representing the percentage of samples to assign to be assigned
        to train and test set, respectively. The remaining will be assigned to the test set. 
        
        `default: (0.7,0.8)` to assign 70% samples to train set, 10% samples to val set and 10% samples to test set
    shuffle: tuple
        Tuple of a bool value representing a flag to decide to if the shuffle should be applied or not (first element of the tuple),
        and an int value representing the shuffle seed (second element of the tuple).
        
        `default: (True,42)`
    
    Returns
    -------
    [train_X, train_y] : list
        List of array of float values representing the input (first element of the list) and 3D array representing the output (second element of the list) in the training set
    [val_X, val_y] : list
        List of array of float values representing the input (first element of the list) and 3D array representing the output (second element of the list) in the validation set
    [test_X, test_y] : list
        List of array of float values representing the input (first element of the list) and 3D array representing the output (second element of the list) in the test set 
    """
    idx_array = np.arange(0, X.shape[0], 1, dtype=int)

    if shuffle[0]:
        np.random.seed(shuffle[1])
        np.random.shuffle(idx_array)
    
    train_idx, val_idx, test_idx = np.split(idx_array, [int(splitting_ratio[0] * X.shape[0]), int(splitting_ratio[1] * X.shape[0])])

    train_X, train_y = X[train_idx], y[train_idx,:,:]
    val_X, val_y = X[val_idx], y[val_idx,:,:]
    test_X, test_y = X[test_idx], y[test_idx,:,:]

    return [train_X, train_y], [val_X, val_y], [test_X, test_y]

def derivative(f,x):
    """
    Compute derivative of 'f' in 'x'.

    Parameters
    ----------
    f : Callable
    x : float

    Return
    ------
    Float value representing the derivative of 'f' evaluated in 'x'
    """
    if x == 0:
        h = 1e-8
    else:
        # A small value compared to x
        h = 1e-8*x
    return (f(x+h)-f(x))/h

def solver(f, f0, x0, epsilon, max_iter):
    """
    Find the value 'xn' such that, when fed to 'f', the output
    is in a distance less than 1e-5 with respect to 'f0'.

    Parameters
    ----------
    f : Callable
    f0 : float
        Target value
    x0 : float
        Starting value to feed in input to 'f'
    epsilon : float
        Maximum allowable distance between 'f(xn)' and 'f0'
    max_iter : int
        Maximum number of iterations to look for 'xn'

    Returns
    -------
    xn : float
        Value such that, when fed to 'f', the output is
        in a distance less than 'epsilon' with respect to 'f'
    """
    xn=x0
    for n in range(0, max_iter):
        y = f(xn)-f0
        if(abs(y)) < epsilon:
           return xn
        slope = derivative(f,xn)
        if(slope == 0):
            return None
        xn = xn-y/slope
    return xn

def get_DeltaR(C):
    """
    Implement simplified expression for radiative forcing relative to 
    pre-industrial (1750) levels by changes in surface air mole fractions of CO2
    reported in Table 3 of https://doi.org/10.5194/gmd-13-3571-2020.

    Parameters
    ----------
    C : float
        CO2 value

    Returns
    -------
    RF_CO2 : value
        Radiative Forcing value computed for 'C' CO2eq value
        """ 
    a1 = -2.4785e-7 #W m-2 ppm -2
    b1 =  0.00075906 # W m-2 ppm -1
    c1 =  -0.0021492 # W m-2 ppb -0.5
    d1 =  5.2488 # W m-2
    C0 =  277.15 # ppm
    Calpha = C0-b1/(2*a1)
    if C>Calpha:
        alpha_prime = d1 - b1*b1/(4*a1)
    elif C>C0:
        alpha_prime = d1 + a1*(C-C0)*(C-C0)+b1*(C-C0)
    else:
        alpha_prime = d1
    N0 = 273.87 #ppb
    alpha_N2O = c1*np.sqrt(N0)
    RF_CO2 = (alpha_prime+alpha_N2O)*np.log(C/C0)
    
    return RF_CO2

def read_CO2_equivalent(maindir, ssp_scenario, model, withAerosolForcing, start_year_training=1850):
    u"""
    Compute CO2 equivalent values from ERF values that take into account aerosols and several Greenhouse Gases.

    Parameters
    ----------
    maindir : string
        Directory in which the dataset with ERF values is stored
    ssp_scenario : string
        String indicating the SSP scenario (e.g., 'ssp245')
    model : string
        String indicating the reduced-complexity model used to estimate ERF values

        Possible values:
        \t* 'MCE-v1-2'
    withAerosolForcing : bool
        Flag to decide if inlcude aerosols in the ERF values or not

    start_year_training : int, default: 1850
        Starting year from which values have to be loaded.

        It is different than 1850 if the user wants to use a training set starting from a year higher than 1850.

    Returns
    -------
    data : pandas.core.frame.DataFrame
        Dataframe containing years and CO2eq values
    CO2_eq : list
        List of CO2eq values computed from ERF values
    year_list : list
        List of years for which CO2eq were computed
    """
    # If start_year_training > 1850, the data will be read from the first training year that was specified
    start_idx = start_year_training - 1850

    # First file with CO2 equivalent from Zebedee_Nichols
    namefile = 'erf_estimates_with_aerosols_Zebedee_Nichols.csv'
    df = pd.read_csv(maindir + namefile,skiprows = [1])
    mymodel = model #'MAGICCv7.5.1'
    myscenario = ssp_scenario
    if withAerosolForcing:
        myvariable  = 'Effective Radiative Forcing|Greenhouse Gases + Aerosols'
        myData = df[(df['climate_model'] == mymodel) & (df['scenario'] == myscenario) & (df['variable'] == myvariable)]
        DeltaR = myData.iloc[0,6+start_idx:255].values 
    elif not withAerosolForcing:
        myvariable  = 'Effective Radiative Forcing|Greenhouse Gases'
        myData = df[(df['climate_model'] == mymodel) & (df['scenario'] == myscenario) & (df['variable'] == myvariable)]
        DeltaR = myData.iloc[0,6+start_idx:255].values 
        
    CO2_eq = []
    year_list = []
    for i in range(len(DeltaR)):
        CO2_eq.append(solver(get_DeltaR,DeltaR[i],500.,1e-5,50))
        year_list.append(1850 + i)
    
    cells = {'Year': year_list,
            'CO2EQ': CO2_eq}
    data = pd.DataFrame(cells, columns = ['Year', 'CO2EQ'])
    
    return data, CO2_eq, year_list

def compute_values_for_scaling(X_ssp_list):
    """
    Compute highest and lowest data values for each SSP scenario.

    Parameters
    ----------
    X_ssp_list : list
        List containing one list of CO2eq values for each SSP scenario

    Returns
    -------
    return_list : list
        List containing two lists: the first one contains the lowest
        data values for each SSP scenario; the second one contains
        the highest data values for each SSP scenario

    """
    return_list = []

    X_min_list = [min(X_list) for X_list in X_ssp_list]
    X_max_list = [max(X_list) for X_list in X_ssp_list]
    return_list.extend([X_min_list, X_max_list])

    return return_list